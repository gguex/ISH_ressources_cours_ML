{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TsPR9wc5_70_isxcjOj-QL-eBYwzwjqA",
      "authorship_tag": "ABX9TyOXaHBv+e7YLtvdwF05YLML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gguex/ISH_ressources_cours_ML/blob/main/TP11_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP 11 : LSTM et GRU\n",
        "\n",
        "Dans ce TP, nous allons apprendre à créer et entrainer des réseaux de type LSTM et GRU pour créer des modèles génératifs.\n",
        "\n",
        "Dans la première partie, nous allons entrainer un réseau LSTM qui permet de générer du texte \"similaire\" à des données d'entrainement. Nous utiliserons pour cela le premier livre des Misérables.\n",
        "\n",
        "Dans la deuxième partie, nous allons entrainer un réseau GRU qui permet de générer des prénoms féminins ou masculins qui \"sonnent juste\", mais qui n'existent pas nécessairement. Nous utiliserons pour cela le jeu de données de prénoms anglophones trouvé sur http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/ (mais vous pouvez essayer avec d'autres listes de prénoms, ces dernières sont faciles à trouver).\n",
        "\n",
        "Les librairies nécessaires sont les suivantes :"
      ],
      "metadata": {
        "id": "xJftcrf_fBjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqoWHussKYEq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "import string\n",
        "# Permet d'afficher le texte avec une certaine largeur\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enregistrons le dispositif de calcul dans une variable."
      ],
      "metadata": {
        "id": "S8FXdFVtj3Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ksaGk3fkEX-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e124faab-58a2-4ced-c549-6110b62e103f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Génération de textes avec LSTM\n",
        "\n",
        "Dans cette partie, nous allons créer un modèle de génération de textes qui sera entrainé sur le premier livre des Misérables (cf. TP 10). Ce modèle sera constitué de plusieurs couches LSTM successives, et devra être entrainé à prédire le prochain token d'une séquence en fonction des tokens précédents."
      ],
      "metadata": {
        "id": "eXUpfBThkE4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` pour pouvoir générer des exemples d'entrainement.\n",
        "\n",
        "Un entrée/sortie de ce dataset sera constitué de deux séquences, tirées de notre document, de taille `seq_len`. La séquence de sortie sera décalée d'un token sur la droite par rapport à l'entrée (voir l'exemple plus bas). Ainsi, chaque token de la séquence d'entrée sera associé avec le token suivant.\n",
        "\n",
        "Nous allons utiliser un modèle de langage de Spacy pour tokeniser notre document."
      ],
      "metadata": {
        "id": "y0ATSP1nku7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceData(Dataset):\n",
        "    def __init__(self, file_path, nlp, seq_len):\n",
        "\n",
        "        # Le chemin d'accès au fichier\n",
        "        self.file_path = file_path\n",
        "        # La longueur des séquences d'entrée et sortie\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # On ouvre notre fichier et on crée un object Spacy\n",
        "        with open(self.file_path, \"r\") as f:\n",
        "          text = f.read()\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # On découpe notre texte en tokens, sans les espaces\n",
        "        self.tokens = [token.text for token in doc if not token.is_space]\n",
        "\n",
        "        # On enregistre le vocabulaire utilisé, et on crée des dictionnaires\n",
        "        # permettant de transformer chaque token en identifiant numérique,\n",
        "        # ou l'inverse.\n",
        "        self.dictionary = list(set(self.tokens))\n",
        "        self.id2token = {id: token for id, token in enumerate(self.dictionary)}\n",
        "        self.token2id = {token: id for id, token in enumerate(self.dictionary)}\n",
        "\n",
        "        # On transforme notre corpus en une séquence de valeurs numériques\n",
        "        self.token_ids = [self.token2id[w] for w in self.tokens]\n",
        "\n",
        "    # La taille de notre dataset est le nombre de séquences possibles\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids) - self.seq_len\n",
        "\n",
        "    # Un élément de notre corpus sera la séquence id et la séquence id+1.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.token_ids[id:id+self.seq_len]),\n",
        "                torch.tensor(self.token_ids[id+1:id+self.seq_len+1]))"
      ],
      "metadata": {
        "id": "aBf2WJTKEXEO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On charge le modèle de langage de Spacy."
      ],
      "metadata": {
        "id": "l4tegHb-oKGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !spacy download \"en_core_web_sm\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "nX5JqfXwEjyR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre classe `SentenceData`, en utilisant le premier livre de notre corpus et en le divisant en séquences de 10 tokens."
      ],
      "metadata": {
        "id": "HoC3igPr0MuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/book_01.txt\"\n",
        "example_doc = SentenceData(doc_path, nlp, 10)"
      ],
      "metadata": {
        "id": "4vRKSDCwGmnp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_doc.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75oAaZDVX3FK",
        "outputId": "70926984-cfd2-4a90-a383-848b7166c319"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4728"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre document contient 28932 paires de séquences."
      ],
      "metadata": {
        "id": "51HAOH7u0VD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJMYd_sH1XI",
        "outputId": "01c4d09b-7b7e-4fd2-f3f7-6b73c1d9a134"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28932"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut examiner un exemple en particulier, en traduisant les séquences numériques en mots."
      ],
      "metadata": {
        "id": "RX4XIx9v0dpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id = 0\n",
        "input, output = example_doc[sample_id]\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in input]))\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in output]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBb9EO8XHORw",
        "outputId": "c86b4f33-9b69-4f71-b85d-b6e32af0cfe6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 1815 , M. Charles - François - Bienvenu Myriel\n",
            "1815 , M. Charles - François - Bienvenu Myriel was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée un objet `torch.utils.data.Dataloader` pour parcourir notre base de données, avec une taille de batch donnée."
      ],
      "metadata": {
        "id": "lXIhKhPm0nPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "my_dataloader = DataLoader(example_doc, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "rP1RISBZNbzX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est temps de créer notre modèle. Ce dernier prendra notre séquence d'entrée et sera consititué de :\n",
        "\n",
        "* Une couche d'embedding, qui transforme nos identifiants numériques en vecteurs one-hot de taille `n_vocab`, puis en vecteur de taille `embedding_dim`.\n",
        "* `num_layers` couches de LSTM, avec des vecteurs d'état caché et d'état de cellule de taille `hidden_size`. On peut ajouter un dropout aux couches, afin que ces dernières n'apprennent pas le texte entièrement par coeur.\n",
        "* Une couche entièrement connectée, avec `n_vocab` sorties.\n",
        "\n",
        "La sortie de ce réseau donnera les log-odds pour les mots suivants, qui peuvent entre converties en probabilités.\n",
        "\n",
        "Notez que la longueur de la séquence n'a pas besoin d'être précisée ici. Pytorch se chargera de prendre tous les éléments reçus, de \"déplier\" le réseau en fonction de leur nombre, et de calculer les sorties en transmettant les états entre les cellules.\n",
        "\n",
        "Remarquez également que nous précisons avec `batch_first=True`, lors de la création des couches LSTM, que notre batch se situe sur la première dimension de nos tenseurs. Ainsi, Pytorch comprend que la séquence se situe sur la deuxième dimension des tenseurs (voir https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)."
      ],
      "metadata": {
        "id": "cm42-xGY1GmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqGen(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, hidden_size, num_layers):\n",
        "        super(SeqGen, self).__init__()\n",
        "\n",
        "        # On sauve les paramètres dans des attributs\n",
        "        self.n_vocab = n_vocab\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # La couche d'embedding\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=self.n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        # Les couches LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    # La fonction foward peut prendre, en plus des entrées,\n",
        "    # l'état caché et l'état de la cellule utilisé au début de la séquence.\n",
        "    # Par défaut, ces états sont posés comme None.\n",
        "    def forward(self, x, prev_state=None):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "kj65kz_oKnr6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre modèle, avec nombre de mots correspondant à notre vocabulaire, embedding de 128 et 3 couches de LSTM avec états de taille 128."
      ],
      "metadata": {
        "id": "-YhjGZK7_5PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_gen = SeqGen(len(example_doc.dictionary), 128, 128, 3)"
      ],
      "metadata": {
        "id": "bcnKUn8oTO6e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser l'entropie croisée et l'optimisateur Adam pour entrainer notre modèle."
      ],
      "metadata": {
        "id": "LgnEYlT5AQTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(seq_gen.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jPcpxj7OT4VA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On entraine maintenant le modèle. Notez qu'aucun état caché est donné à notre modèle avant chaque batch, ce qui veut dire que les états initiaux seront nuls avant chaque séquence. L'entrainement est relativement simple, car la surparamétrisation n'est pas très importante pour notre modèle."
      ],
      "metadata": {
        "id": "B2581VwZAcd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le nombre d'epochs\n",
        "n_epochs = 30\n",
        "\n",
        "# On met le modèle sur le dispositif de calcul\n",
        "seq_gen.to(device)\n",
        "# On le met en mode entrainement\n",
        "seq_gen.train()\n",
        "# On boucle sur les epochs\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "  # Pour calculer la perte moyenne\n",
        "  sum_loss = 0\n",
        "  # On boucle sur notre dataloader\n",
        "  for input, output in my_dataloader:\n",
        "\n",
        "    # Les entrées et sorties sont mises sur le dispositif de calcul\n",
        "    input = input.to(device)\n",
        "    output = output.to(device)\n",
        "\n",
        "    # On met à zéro les gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # On fait les prédictions\n",
        "    pred, _ = seq_gen(input)\n",
        "    # On calcule la perte, en transposant nos résultats\n",
        "    loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "    # On fait une itération de descente du gradient\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # On ajoute la perte\n",
        "    sum_loss += loss.item()\n",
        "\n",
        "  print(f\"mean loss = {sum_loss / len(my_dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrWiGXq-T7ty",
        "outputId": "95a1a489-92c0-4fbe-d9d0-ace7f3267d6e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean loss = 6.7336\n",
            "Epoch 2: mean loss = 6.2557\n",
            "Epoch 3: mean loss = 6.1583\n",
            "Epoch 4: mean loss = 6.0445\n",
            "Epoch 5: mean loss = 5.8407\n",
            "Epoch 6: mean loss = 5.6218\n",
            "Epoch 7: mean loss = 5.4399\n",
            "Epoch 8: mean loss = 5.2722\n",
            "Epoch 9: mean loss = 5.1181\n",
            "Epoch 10: mean loss = 4.9770\n",
            "Epoch 11: mean loss = 4.8436\n",
            "Epoch 12: mean loss = 4.7203\n",
            "Epoch 13: mean loss = 4.5949\n",
            "Epoch 14: mean loss = 4.4869\n",
            "Epoch 15: mean loss = 4.3746\n",
            "Epoch 16: mean loss = 4.2786\n",
            "Epoch 17: mean loss = 4.1957\n",
            "Epoch 18: mean loss = 4.0979\n",
            "Epoch 19: mean loss = 4.0131\n",
            "Epoch 20: mean loss = 3.9277\n",
            "Epoch 21: mean loss = 3.8473\n",
            "Epoch 22: mean loss = 3.7668\n",
            "Epoch 23: mean loss = 3.6948\n",
            "Epoch 24: mean loss = 3.6273\n",
            "Epoch 25: mean loss = 3.5591\n",
            "Epoch 26: mean loss = 3.4974\n",
            "Epoch 27: mean loss = 3.4331\n",
            "Epoch 28: mean loss = 3.3719\n",
            "Epoch 29: mean loss = 3.3125\n",
            "Epoch 30: mean loss = 3.2590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va maintenant donner à notre modèle un début de séquence et voir comment il génère une suite. Le processus se passe en deux étapes :\n",
        "\n",
        "* On passe une première fois la séquence initiale dans notre modèle, afin de générer le prochain token et pour récupérer l'état caché résultant.\n",
        "* On va ensuite faire une boucle pour les tokens restants, en passant dans le réseau le dernier token et états obtenus à l'étape précédente.\n",
        "\n",
        "Pour générer chaque nouveau token, on transforme les log-odds en probabilités, puis on effectue un tirage aléatoire selon ces dernières.\n",
        "\n",
        "Le séquence finale est affichée formattée."
      ],
      "metadata": {
        "id": "P2pM7sEdGYaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Séquence initiale\n",
        "input_sentence = \"When the man\"\n",
        "# Nombre de tokens désiré\n",
        "n_generated_tokens = 600\n",
        "\n",
        "# --- Etape 1\n",
        "\n",
        "# On coupe notre séquence intiale pour créer notre séquence de sortie\n",
        "output_tokens = input_sentence.split()\n",
        "# On met le modèle en mode évaluation\n",
        "seq_gen.eval()\n",
        "# Notre séquence d'entrée est transformée en identifiants numériques\n",
        "input = torch.tensor([example_doc.token2id[token]\n",
        "                      for token in output_tokens]).to(device)\n",
        "# On passe nos entrées dans notre modèle, en ne donnant aucun état.\n",
        "pred, hidden = seq_gen(input)\n",
        "# On garde les logodds du dernier mot. Notez que nous utilisons\n",
        "# detach().cpu() pour ne pas garder le graph de calcul et pour\n",
        "# basculer le tenseur sur le cpu.\n",
        "new_token_logodds = pred.detach().cpu()[-1]\n",
        "# Les probabilités sont calculées avec un softmax\n",
        "probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "# On tire aléatoirement le token suivant, avec les probabilités précédentes\n",
        "token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "# On l'ajoute à notre sortie\n",
        "output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# --- Etape 2\n",
        "\n",
        "# On boucle sur le nombre restant de tokens demandés\n",
        "for i in range(n_generated_tokens - 1):\n",
        "  # Notre dernier token est transformé en entrée\n",
        "  input = torch.tensor([example_doc.token2id[output_tokens[-1]]]).to(device)\n",
        "  # On passe notre entrée, avec les états précédants, dans notre modèle\n",
        "  pred, hidden = seq_gen(input, hidden)\n",
        "  # On détache, met sur le cpu et applatit la sortie\n",
        "  new_token_logodds = pred.detach().cpu().flatten()\n",
        "  # Les probabilités sont calculées avec un softmax\n",
        "  probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "  # On tire aléatoirement, avec les probabilités calculées, le token suivant\n",
        "  token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "  # On l'ajoute à notre sortie\n",
        "  output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# On affiche notre sortie formatée\n",
        "print(textwrap.fill(\" \".join(output_tokens), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0JzgJA9VfIk",
        "outputId": "8b6b36ed-443d-4fb6-ecd2-9a71ca34be30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the man ! It was there is a atheist for his adherence . Moreover , her\n",
            "room put a politician , wears he redoubled offers little upon the remark of\n",
            "instinct , with directly at the father of life , there know a heart on his\n",
            "decrepit - charming ; this mystery , seemed ; as a form which is alarming , as\n",
            "we have done out , opening more authorized to traits , how an excuse , rather\n",
            "Cassette , although if the people to the recapitulation of purple ; this\n",
            "natures francs out . While they must caused three ’s d’Assisi and perfectly ill\n",
            "; with Waterloo who should been utterly wounded for him , and on charming\n",
            "towards young people is a comprehend to a leisure of gold , proscribed to\n",
            "stating he ! No one passed himself , frightful saying of slavery ; of other\n",
            "over the house , in life , we have sometimes approaches in perfection , and\n",
            "errors like him to adore children . Twenty days and reckoning ; it was very\n",
            "women at her conclusion . ” The chambers steadily from the moment of touching\n",
            "or God . The scaffold is so turn to be out , in gentle somewhat for creation ,\n",
            "sir , equitable , Saint radiated again ; in words , where even himself , but\n",
            "than widely all to happened to a village man , which took suffering at a heart\n",
            "; it seemed with an gallican ? That my name adversaries ! be infantile for\n",
            "something incessantly ; and digs posts ; it slipped for fragments , below ,\n",
            "quite formations , which was distinctly more of behind himself , gradually , in\n",
            "its senses . He was dazzled formed as though that of saying , not dazzles\n",
            "indulgent hundred opening into its diocese ; the drawing - father of\n",
            "incapacitated , contemplate in advance , and responsibility ; so vocation in a\n",
            "rock , and if it is there . He did not shake in that things , where transcribe\n",
            "him Bienvenu enclosure , in another indirectly recommended . What did not known\n",
            "it . Certainly , filled that or rumors without pork ; let this newcomer that\n",
            "what will flight by useful to our ignorant ’s violent full of milk . What am\n",
            "something ? There is there ! Jourdan towards clever place , willingly of the\n",
            "very age ; he forgot , perhaps , men in unity , comparing the pause , hence a\n",
            "prefect . With this tenderness in Stephen , who will not have mean in gentle ,\n",
            "on that gardens , cursed , necessity , flowed in liberty , but which went\n",
            "inquiring as a amiable harshness in strange ; him , in finding men men men from\n",
            "alms , there applauds that three morning to his bishop no eels for every\n",
            "chanting those against soul . These battle is deciphering by hostile , over to\n",
            "combine and Jeanne must one bullion , by his hour of gold . A vicissitudes of\n",
            "straw - neck ; the apostleship had been him to be neutral . On the name of a\n",
            "life ; not even from a immense cloth above and rainy virtue of furniture , and\n",
            "repeated to darkness , my incurable virtue of works who thinks before him a\n",
            "abbés of aspect , and understood . Was the father , an beaming of a great\n",
            "conviction . . . . have said his result : I have spoken , liberty of accordance\n",
            "by dangerous bearings in men which produces can one considered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "weevWLMZ9bQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Génération de prénoms avec GRU\n",
        "\n",
        "Maintenant, nous allons utiliser les GRU pour créer des modèles de génération de prénoms, en utilisant des données de prénoms féminins et masculins. Le principe reste assez similaire à la partie précédente, mais nous adopterons une programmation de style plus fonctionnel."
      ],
      "metadata": {
        "id": "_zoqTHMJLtZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` qui va donner des exemples constitués d'un nom comme entrée et du même nom décalé d'une lettre sur la droite comme sortie. Comme caractère de fin de séquence et de padding, nous allons utiliser le caractère `\"\\n\"` déjà présent à la fin de chaque nom dans le fichier (sauf du dernier). Nous effectuons un padding sur tous les prénoms afin que tous fassent la taille du plus long prénom (pour entrainer les modèles avec des batchs)."
      ],
      "metadata": {
        "id": "qPy0pb8TMCsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameData(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        # On enregistre le chemin du fichier\n",
        "        self.file_path = file_path\n",
        "        # On ouvre le fichier et on lit les lignes\n",
        "        with open(file_path, \"r\") as f:\n",
        "          self.lines = f.readlines()\n",
        "        # On ajoute notre caractère de padding à la dernière ligne\n",
        "        self.lines[-1] += \"\\n\"\n",
        "        # La longueur maximale est la longueur de la plus grande ligne\n",
        "        self.max_len = max([len(line) for line in self.lines])\n",
        "        # Les lignes sont complétées avec \"\\n\" pour avoir la même taille\n",
        "        self.lines = [line + \"\\n\"*(self.max_len - len(line))\n",
        "                      for line in self.lines]\n",
        "        # On met les lignes en minuscules\n",
        "        self.lines_low = [line.lower() for line in self.lines]\n",
        "        # On sauve le nombre de lettres différentes\n",
        "        self.letters = list(set(\"\".join(self.lines_low)))\n",
        "        # On crée notre liste de nom, avec chaque lettre\n",
        "        # comme élément d'une liste\n",
        "        self.names = [list(line) for line in self.lines_low]\n",
        "\n",
        "        # Ces fonctions permettent de transformer les lettres en identifiants\n",
        "        # numériques, ou vice-versa.\n",
        "        self.id2letter = {id: letter for id, letter in enumerate(self.letters)}\n",
        "        self.letter2id = {letter: id for id, letter in enumerate(self.letters)}\n",
        "\n",
        "        # On sauve l'identifiant du caractère de padding\n",
        "        self.endl_id = self.letter2id[\"\\n\"]\n",
        "\n",
        "        # On transforme nos listes de lettres en listes d'ids\n",
        "        self.names_ids = [[self.letter2id[l] for l in name]\n",
        "                          for name in self.names]\n",
        "\n",
        "    # La longueur du dataset\n",
        "    def __len__(self):\n",
        "        return len(self.names_ids)\n",
        "\n",
        "    # Un item est une entrée/sortie avec, respectivement,\n",
        "    # prénom sans la dernière lettre, prénom sans la première lettre.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.names_ids[id][:-1]),\n",
        "                torch.tensor(self.names_ids[id][1:]))"
      ],
      "metadata": {
        "id": "2PLVeGU39cC4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée maintenant nos instances contenant nos jeux de données."
      ],
      "metadata": {
        "id": "BpFQh1DkOrJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/female.txt\"\n",
        "male_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/male.txt\"\n",
        "female_data = NameData(female_path)\n",
        "male_data = NameData(male_path)"
      ],
      "metadata": {
        "id": "ukgFs2NC9yMT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons un exemple."
      ],
      "metadata": {
        "id": "9OLmhvMiO0ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, output = female_data[0]\n",
        "print([female_data.id2letter[id.item()] for id in input])\n",
        "print([female_data.id2letter[id.item()] for id in output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYDL5IPb94bZ",
        "outputId": "97ccb786-638a-474a-9324-b7dbad30abc9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n",
            "['b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous créons maintenant la classe de notre modèle. Ce dernier passe  directement les vecteurs one-hot dans les couches GRU, car le nombre de caractères existants n'est pas très élevé (la dimensionnalité des vecteurs one-hot est déjà basse). Le modèle est constitué de :\n",
        "\n",
        "* `num_layer` couches GRU avec états cachés de taille `hidden_size`.\n",
        "* Une couche entièrement connectée qui envoie les états cachés finaux sur la taille du vocabulaire.\n",
        "\n",
        "Le résultat du modèle sont les log-odds des prochains caractères."
      ],
      "metadata": {
        "id": "XE6CYho4jfOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameGen(nn.Module):\n",
        "    def __init__(self, n_vocab, hidden_size, num_layers):\n",
        "        super(NameGen, self).__init__()\n",
        "\n",
        "        # On enregistre les paramètres du modèle\n",
        "        self.n_vocab = n_vocab\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Les couches GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.n_vocab,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "        # Cette fonction transforme les entiers en vecteurs one-hot\n",
        "        one_hot = nn.functional.one_hot(x, num_classes=self.n_vocab)\n",
        "        # On passe dans les couches GRU\n",
        "        output, state = self.gru(one_hot.to(torch.float), prev_state)\n",
        "        # Les log-odds résultantes\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "zQ0fLqLc-Eab"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous instancions un modèle et lui passons un exemple, pour voir si tout fonctionne."
      ],
      "metadata": {
        "id": "ZjaDToc6lQWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 20, 2)\n",
        "female_name_gen(input)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV5nAhek-FDs",
        "outputId": "016b323d-d131-4af6-c333-00755142ba2c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2625e-01, -1.3404e-01,  1.9273e-01,  7.4753e-02,  3.2473e-02,\n",
              "         -5.3528e-02, -3.8129e-02, -2.7323e-01, -8.8352e-02, -3.2072e-02,\n",
              "          8.1128e-02, -1.8301e-01, -1.1507e-01,  8.1924e-02, -1.2629e-01,\n",
              "         -1.8091e-01,  2.0811e-01,  2.0678e-01,  7.6976e-02, -2.4153e-01,\n",
              "          7.4301e-02, -2.5053e-01,  4.4130e-02, -2.6339e-01, -6.0192e-03,\n",
              "         -2.4544e-02, -1.1928e-01,  3.2498e-02,  9.9209e-02,  3.1888e-02],\n",
              "        [ 1.5580e-01, -1.3477e-01,  2.0880e-01,  7.3884e-02,  2.0569e-02,\n",
              "         -8.9267e-02,  1.0815e-02, -2.9651e-01, -8.7315e-02, -9.2361e-03,\n",
              "          4.0939e-02, -1.8183e-01, -1.2900e-01,  2.3720e-03, -9.1838e-02,\n",
              "         -1.5401e-01,  2.3657e-01,  2.1741e-01,  7.9559e-02, -2.5496e-01,\n",
              "          5.2388e-02, -2.9050e-01,  2.4339e-02, -2.9354e-01,  1.0409e-03,\n",
              "         -2.2507e-02, -1.6569e-01,  4.8507e-03,  6.3490e-02,  1.3062e-02],\n",
              "        [ 1.7277e-01, -1.3440e-01,  2.1694e-01,  6.5793e-02,  2.2751e-02,\n",
              "         -1.1315e-01,  3.5149e-02, -3.1978e-01, -1.0805e-01,  1.7959e-02,\n",
              "          1.5682e-02, -1.8547e-01, -1.1982e-01, -3.3421e-02, -8.6006e-02,\n",
              "         -1.5353e-01,  2.3689e-01,  2.2404e-01,  9.8972e-02, -2.7830e-01,\n",
              "          3.1449e-02, -3.0465e-01,  6.7216e-03, -3.1718e-01,  1.7074e-02,\n",
              "         -2.2230e-02, -1.8676e-01,  1.2528e-02,  4.9559e-02, -2.1252e-02],\n",
              "        [ 1.8472e-01, -1.2638e-01,  2.2173e-01,  6.0563e-02,  3.2172e-02,\n",
              "         -1.2316e-01,  4.5533e-02, -3.1901e-01, -1.2126e-01,  2.5184e-02,\n",
              "          6.0181e-03, -1.9871e-01, -1.2752e-01, -4.8001e-02, -7.6231e-02,\n",
              "         -1.6219e-01,  2.4297e-01,  2.3001e-01,  1.0165e-01, -2.9112e-01,\n",
              "          3.3513e-02, -3.2106e-01, -2.1262e-03, -3.3715e-01,  2.1836e-02,\n",
              "         -1.7094e-02, -1.9414e-01,  6.9510e-03,  4.9418e-02, -3.7740e-02],\n",
              "        [ 1.9245e-01, -1.2531e-01,  2.2023e-01,  5.5394e-02,  3.5467e-02,\n",
              "         -1.3047e-01,  5.0087e-02, -3.3027e-01, -1.3671e-01,  3.6366e-02,\n",
              "         -3.5369e-03, -2.0693e-01, -1.1709e-01, -5.7601e-02, -8.0945e-02,\n",
              "         -1.6681e-01,  2.3607e-01,  2.3380e-01,  1.1697e-01, -3.0242e-01,\n",
              "          2.5057e-02, -3.2221e-01, -5.8402e-03, -3.4571e-01,  3.1623e-02,\n",
              "         -1.9922e-02, -2.0128e-01,  1.9538e-02,  4.7730e-02, -5.7668e-02],\n",
              "        [ 2.0262e-01, -1.2067e-01,  2.2852e-01,  3.7970e-02,  2.7649e-02,\n",
              "         -1.3717e-01,  6.1156e-02, -3.4238e-01, -1.3918e-01,  1.4289e-02,\n",
              "          7.0252e-05, -2.1474e-01, -1.3132e-01, -5.9974e-02, -7.6723e-02,\n",
              "         -1.5762e-01,  2.3906e-01,  2.4093e-01,  1.1320e-01, -3.0854e-01,\n",
              "          3.8020e-02, -3.3663e-01, -2.3511e-03, -3.4756e-01,  3.1178e-02,\n",
              "         -1.3586e-02, -2.1856e-01, -7.1872e-03,  4.7206e-02, -6.8084e-02],\n",
              "        [ 1.9635e-01, -1.1702e-01,  2.2544e-01,  2.1595e-02,  1.0702e-02,\n",
              "         -1.3252e-01,  6.0225e-02, -3.5268e-01, -1.4870e-01, -9.2535e-03,\n",
              "          4.9926e-03, -2.2115e-01, -1.2636e-01, -5.2531e-02, -7.7190e-02,\n",
              "         -1.3709e-01,  2.4947e-01,  2.4299e-01,  1.0940e-01, -3.0449e-01,\n",
              "          5.1999e-02, -3.3903e-01,  1.5414e-02, -3.3801e-01,  3.0092e-02,\n",
              "         -6.4666e-03, -2.3680e-01, -2.2997e-02,  3.2543e-02, -7.0339e-02],\n",
              "        [ 2.0123e-01, -1.0886e-01,  2.2286e-01,  2.0072e-02, -6.8119e-03,\n",
              "         -1.3560e-01,  6.6731e-02, -3.6964e-01, -1.2746e-01, -2.9098e-02,\n",
              "         -1.1946e-02, -2.2141e-01, -1.2073e-01, -7.5810e-02, -9.3101e-02,\n",
              "         -1.0851e-01,  2.5247e-01,  2.7079e-01,  1.0038e-01, -3.0741e-01,\n",
              "          4.5210e-02, -3.4133e-01,  4.5367e-02, -3.1957e-01,  4.7628e-03,\n",
              "         -3.0749e-02, -2.7387e-01, -5.1762e-02,  2.4750e-02, -7.3144e-02],\n",
              "        [ 2.0748e-01, -1.0448e-01,  2.2638e-01,  2.4700e-02, -2.0776e-02,\n",
              "         -1.4123e-01,  7.5711e-02, -3.8207e-01, -1.0179e-01, -4.0194e-02,\n",
              "         -3.3028e-02, -2.1453e-01, -1.1715e-01, -1.0251e-01, -1.0556e-01,\n",
              "         -8.6275e-02,  2.5630e-01,  2.9544e-01,  9.1006e-02, -3.1287e-01,\n",
              "          3.4827e-02, -3.4275e-01,  6.2498e-02, -3.0731e-01, -2.0478e-02,\n",
              "         -5.2083e-02, -3.0615e-01, -7.7394e-02,  1.6632e-02, -7.6214e-02],\n",
              "        [ 2.1186e-01, -1.0371e-01,  2.3328e-01,  3.0232e-02, -3.0238e-02,\n",
              "         -1.4707e-01,  8.4512e-02, -3.8951e-01, -8.0751e-02, -4.4414e-02,\n",
              "         -5.1069e-02, -2.0472e-01, -1.1564e-01, -1.2431e-01, -1.1133e-01,\n",
              "         -7.1752e-02,  2.6101e-01,  3.1148e-01,  8.3701e-02, -3.1781e-01,\n",
              "          2.6253e-02, -3.4371e-01,  6.7951e-02, -3.0147e-01, -3.8997e-02,\n",
              "         -6.4796e-02, -3.2865e-01, -9.6177e-02,  8.3029e-03, -7.8781e-02],\n",
              "        [ 2.1415e-01, -1.0492e-01,  2.4087e-01,  3.4592e-02, -3.5775e-02,\n",
              "         -1.5232e-01,  9.1998e-02, -3.9365e-01, -6.6054e-02, -4.4646e-02,\n",
              "         -6.4416e-02, -1.9512e-01, -1.1545e-01, -1.3976e-01, -1.1225e-01,\n",
              "         -6.3200e-02,  2.6527e-01,  3.2058e-01,  7.8939e-02, -3.2140e-01,\n",
              "          2.0438e-02, -3.4442e-01,  6.7019e-02, -2.9967e-01, -5.0829e-02,\n",
              "         -7.0922e-02, -3.4250e-01, -1.0871e-01,  1.0696e-03, -8.0669e-02],\n",
              "        [ 2.1504e-01, -1.0684e-01,  2.4761e-01,  3.7435e-02, -3.8521e-02,\n",
              "         -1.5670e-01,  9.7836e-02, -3.9596e-01, -5.6721e-02, -4.3102e-02,\n",
              "         -7.3570e-02, -1.8717e-01, -1.1592e-01, -1.4984e-01, -1.1067e-01,\n",
              "         -5.8592e-02,  2.6839e-01,  3.2535e-01,  7.6287e-02, -3.2372e-01,\n",
              "          1.6948e-02, -3.4497e-01,  6.3670e-02, -2.9987e-01, -5.7765e-02,\n",
              "         -7.3298e-02, -3.5022e-01, -1.1662e-01, -4.3833e-03, -8.1945e-02],\n",
              "        [ 2.1522e-01, -1.0876e-01,  2.5292e-01,  3.9070e-02, -3.9576e-02,\n",
              "         -1.6018e-01,  1.0211e-01, -3.9734e-01, -5.1177e-02, -4.1089e-02,\n",
              "         -7.9576e-02, -1.8118e-01, -1.1663e-01, -1.5604e-01, -1.0824e-01,\n",
              "         -5.6338e-02,  2.7032e-01,  3.2771e-01,  7.5080e-02, -3.2511e-01,\n",
              "          1.5077e-02, -3.4540e-01,  6.0035e-02, -3.0085e-01, -6.1571e-02,\n",
              "         -7.3907e-02, -3.5409e-01, -1.2146e-01, -8.0647e-03, -8.2742e-02],\n",
              "        [ 2.1513e-01, -1.1038e-01,  2.5679e-01,  3.9921e-02, -3.9750e-02,\n",
              "         -1.6283e-01,  1.0507e-01, -3.9826e-01, -4.8050e-02, -3.9235e-02,\n",
              "         -8.3406e-02, -1.7696e-01, -1.1738e-01, -1.5971e-01, -1.0583e-01,\n",
              "         -5.5374e-02,  2.7134e-01,  3.2883e-01,  7.4729e-02, -3.2587e-01,\n",
              "          1.4210e-02, -3.4572e-01,  5.6986e-02, -3.0199e-01, -6.3536e-02,\n",
              "         -7.3840e-02, -3.5576e-01, -1.2436e-01, -1.0336e-02, -8.3195e-02],\n",
              "        [ 2.1501e-01, -1.1163e-01,  2.5945e-01,  4.0324e-02, -3.9551e-02,\n",
              "         -1.6476e-01,  1.0705e-01, -3.9893e-01, -4.6361e-02, -3.7763e-02,\n",
              "         -8.5804e-02, -1.7411e-01, -1.1805e-01, -1.6182e-01, -1.0381e-01,\n",
              "         -5.5058e-02,  2.7176e-01,  3.2934e-01,  7.4810e-02, -3.2624e-01,\n",
              "          1.3909e-02, -3.4595e-01,  5.4725e-02, -3.0300e-01, -6.4484e-02,\n",
              "         -7.3602e-02, -3.5629e-01, -1.2609e-01, -1.1632e-02, -8.3417e-02]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'idée de prendre une approche fonctionnelle, nous définissons ici la fonction qui permet d'entrainer un modèle."
      ],
      "metadata": {
        "id": "idkOpfztle6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data, batch_size, n_epochs, device):\n",
        "\n",
        "  # On définit la fonction de perte et l'optimisateur\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  # On crée le dataloader\n",
        "  dataloader = DataLoader(data, batch_size, shuffle=True)\n",
        "\n",
        "  # On bascule le modèle en mode entrainement\n",
        "  model.train()\n",
        "  # On le met sur le dispositif de calcul\n",
        "  model.to(device)\n",
        "  # La boucle d'entrainement\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "    # Pour calculer la perte moyenne\n",
        "    sum_loss = 0\n",
        "    # La boucle sur les batchs\n",
        "    for input, output in dataloader:\n",
        "\n",
        "      # On met l'entrée et la sortie sur le dispositif de calcul\n",
        "      input = input.to(device)\n",
        "      output = output.to(device)\n",
        "\n",
        "      # On met les gradients à zéro, on fait des prédictions et on\n",
        "      # calcule la perte\n",
        "      optimizer.zero_grad()\n",
        "      pred, _ = model(input)\n",
        "      loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "      # On effectue la descente du gradient et on cumule les pertes\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      sum_loss += loss.item()\n",
        "\n",
        "    print(f\"mean Loss = {sum_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "s1xFZYsw-L1S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise maintenant la fonction créée pour entrainer un modèle avec les prénoms féminins."
      ],
      "metadata": {
        "id": "WU9xzgalmgBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 30, 3)\n",
        "train_model(female_name_gen, female_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUUZldXv-Q08",
        "outputId": "e421a091-71aa-47ab-bd73-eac66346ec14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.3638\n",
            "Epoch 2: mean Loss = 0.9861\n",
            "Epoch 3: mean Loss = 0.9136\n",
            "Epoch 4: mean Loss = 0.8787\n",
            "Epoch 5: mean Loss = 0.8607\n",
            "Epoch 6: mean Loss = 0.8464\n",
            "Epoch 7: mean Loss = 0.8332\n",
            "Epoch 8: mean Loss = 0.8210\n",
            "Epoch 9: mean Loss = 0.8115\n",
            "Epoch 10: mean Loss = 0.8018\n",
            "Epoch 11: mean Loss = 0.7920\n",
            "Epoch 12: mean Loss = 0.7805\n",
            "Epoch 13: mean Loss = 0.7682\n",
            "Epoch 14: mean Loss = 0.7592\n",
            "Epoch 15: mean Loss = 0.7509\n",
            "Epoch 16: mean Loss = 0.7446\n",
            "Epoch 17: mean Loss = 0.7373\n",
            "Epoch 18: mean Loss = 0.7313\n",
            "Epoch 19: mean Loss = 0.7252\n",
            "Epoch 20: mean Loss = 0.7194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "eg4j_PXPmpOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_name_gen = NameGen(len(male_data.letters), 30, 3)\n",
        "train_model(male_name_gen, male_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PthBaFgw-TIH",
        "outputId": "3330e272-1270-4cf9-8f96-1d8e6b7b81e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.5468\n",
            "Epoch 2: mean Loss = 1.1056\n",
            "Epoch 3: mean Loss = 1.0434\n",
            "Epoch 4: mean Loss = 1.0212\n",
            "Epoch 5: mean Loss = 1.0033\n",
            "Epoch 6: mean Loss = 0.9690\n",
            "Epoch 7: mean Loss = 0.9350\n",
            "Epoch 8: mean Loss = 0.9218\n",
            "Epoch 9: mean Loss = 0.9139\n",
            "Epoch 10: mean Loss = 0.9083\n",
            "Epoch 11: mean Loss = 0.9000\n",
            "Epoch 12: mean Loss = 0.8899\n",
            "Epoch 13: mean Loss = 0.8797\n",
            "Epoch 14: mean Loss = 0.8702\n",
            "Epoch 15: mean Loss = 0.8603\n",
            "Epoch 16: mean Loss = 0.8516\n",
            "Epoch 17: mean Loss = 0.8439\n",
            "Epoch 18: mean Loss = 0.8365\n",
            "Epoch 19: mean Loss = 0.8310\n",
            "Epoch 20: mean Loss = 0.8242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons maintenant une fonction qui permet de générer un prénom en fonction des premiers caractères. La procédure est très similaire à la génération de tokens, mais on va arrêter la génération dès que notre modèle prédit le caractère \"\\n\", signifiant la fin de séquence."
      ],
      "metadata": {
        "id": "sepYp7C9snFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_name(init_letters, model, data, device):\n",
        "\n",
        "  # --- Etape 1\n",
        "\n",
        "  output_letters = list(init_letters)\n",
        "  model.eval()\n",
        "  input = torch.tensor([data.letter2id[letter]\n",
        "                        for letter in output_letters]).to(device)\n",
        "  pred, hidden = model(input)\n",
        "  new_letter_logodds = pred.detach().cpu()[-1]\n",
        "  probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "  next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "  output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  # --- Etape 2\n",
        "\n",
        "  # On boucle tant que l'on a pas généré \"\\n\"\n",
        "  while not output_letters[-1] == \"\\n\":\n",
        "    input = torch.tensor([data.letter2id[output_letters[-1]]]).to(device)\n",
        "    pred, hidden = model(input, hidden)\n",
        "    new_letter_logodds = pred.detach().cpu().flatten()\n",
        "    probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "    next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "    output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  return \"\".join(output_letters)[:-1]"
      ],
      "metadata": {
        "id": "m_YmXApw-WpE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testons la fonction de génération avec un début de prénom."
      ],
      "metadata": {
        "id": "X_cRCEZB72vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_name(\"su\", female_name_gen, female_data, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OWuQx8Urr_Qo",
        "outputId": "8627f9f4-de2e-42b6-9228-a8397d6593a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sules'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons une liste conséquente de prénoms féminins, pour toutes les lettres de l'alphabet."
      ],
      "metadata": {
        "id": "NA412zGe8EVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "female_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_female_names = []\n",
        "  for _ in range(n_gen):\n",
        "    female_name = generate_name(init_letter, female_name_gen,\n",
        "                                female_data, device)\n",
        "    letter_female_names.append(female_name)\n",
        "\n",
        "  female_names.extend(letter_female_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_female_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpF-Pgip-Y2Y",
        "outputId": "0f260f5f-6c3f-417e-c3ea-b9d51ebe6545"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: aley, alin, ally, alna, ailan, argile, aitta, aloril, ailil, aurrin\n",
            "b: britte, bymcrine, blynna, bandel, bhianna, bmenni, bep, ben, byl, barlie\n",
            "c: cierrie, carry, charleda, cionetto, crossia, cebline, cibarena, cabsette, cicki, chernilina\n",
            "d: domyda, dara, dronna, dapty, denci, drani, damesty, danca, denia, dins\n",
            "e: elvi, elly, elis, eudrandie, evenity, emarica, eis, eridis, elinesh, evee\n",
            "f: feitta, ferella, flenolene, forneta, fiddi, fronny, fabhin, flemine, faresnee, forise\n",
            "g: granetta, gomelah, gorciann, gortty, gaiona, goleta, gevalisa, gabry, glauriane, glivette\n",
            "h: heortien, hartis, harette, harli, heytta, harlent, hilli, huda, haauna, harola\n",
            "i: iie, ivhanie, id, istytha, ieandra, innica, isticfet, isbellie, inoretta, ilygmanne\n",
            "j: joriegara, jiandelia, jorola, jaba, jevatey, jacque, joceana, jindala, jerviane, jistian\n",
            "k: kar, kanieel, ka, kedy, karetha, kermirie, kerri, kelli, krusty, kaian\n",
            "l: lufaan, loree, laume, leanjeen, lrelyh, liy, loryn, luti, lorora, lorca\n",
            "m: merie, mayle, miysher, mecly, morotabena, mayra, melvienna, meort, mica, misni\n",
            "n: nacue, nalionne, neflee, napelle, nareli, nesla, nannine, nylli, norica, nasya\n",
            "o: olisal, omonna, ophelly, othalia, oledia, olidie, ogmela, oreline, onette, olelah\n",
            "p: pbigylne, pathela-ia, phequycha, prina, pilly, phallene, pish, penderla, pealdh, pxithe\n",
            "q: quarcel, queve, quedin, qunea, quybannde, quillita, quenettine, quyleet, qrolie, qorora\n",
            "r: rulie, reta, rofy, reanona, remdee, rocky, relena, race, roe, reoleny\n",
            "s: saribyk, shetdiana, sherestrea, siskin, sarmonie, sikis, snannia, somy, seby, sheel\n",
            "t: tercie, tenita, thiine, tinia, tulvy, tazra, tawita, thaymedt, tarilla, tarmi\n",
            "u: urinie, umeona, uncona, ulyz, ulende, udiah, urissa, ubra, ureelca, umi\n",
            "v: varba, volgian, vaurrra, verla, velly, vea, vanmey, veilma, varena, veryllna\n",
            "w: wirtina, wylis, warna, wy, wir, wanni, winni, waydel, wor, winderynn\n",
            "x: xebarah, xhandrea, xil, xona, xizie, xadba, xannen, xina, xelonina, xaite\n",
            "y: ylaza, yvanetth, ynka, yloris, yliana, yliana, ycelole, ylanette, yphey, yctry\n",
            "z: zare, zenette, zina, zaree, zalda, zrana, zrosa, zimela, zrenda, zanett\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "XajfC7vu8PtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "male_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_male_names = []\n",
        "  for _ in range(n_gen):\n",
        "    male_name = generate_name(init_letter, male_name_gen,\n",
        "                              male_data, device)\n",
        "    letter_male_names.append(male_name)\n",
        "\n",
        "  male_names.extend(letter_male_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_male_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9LK-nF6-bHb",
        "outputId": "a073e2ce-31d3-4f45-f945-762727a38475"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: axetor, adles, an, akaler, abrey, als, an, aleen, aniy, aline\n",
            "b: bemdxasor, bantan, borre, barro, briilit, baunlus, barsgel, burcieluy, banje, baray\n",
            "c: chysos, colley, cenmer, conry, cjedr, cawsen, cibmaugit, chackan, clallan, cont\n",
            "d: digrie, danvy, duickoni, dabs, donray, dertos, dernalo, dradife, dopn, debrzet\n",
            "e: engon, eckine, eudussy, eburi, erord, efbraedard, ellen, eolroth, enitollet, endarn\n",
            "f: fmaelarn, fraoufderd, funne, fartin, furcy, florctard, fanie, fharride, fhillomao, fratpie\n",
            "g: gimie, gareles, gerne, gannin, goscer, gestere, gaursg, gherril, gathan, gebry\n",
            "h: hinfieman, harmipl, hanceo, hadmov, handanter, hlemebos, hornsem, histy, harnie, haly\n",
            "i: iviel, irex, irshed, iljis, ingarnie, ilberta, ivili, ilf, ilricie, iwatort\n",
            "j: jaurie, jaxel, jaatan, jembert, jeflarsond, jaston, jusothy, jomy, jark, jarion\n",
            "k: kusko, kostie, kadie, kisteri, kim, koeof, kande, k, kelshethie, kace\n",
            "l: largy, lunseld, ltauney, lament, lamlan, lanriko, lorny, lutf, lebbuod, letvie\n",
            "m: mooune, madbeno, mirty, mersor, marshell, marnie, murad, mranlelo, memartaugk, medris\n",
            "n: nely, nourasthos, nietrice, naruron, nenoradd, narcon, nooedr, nistog, nolenfi, natel\n",
            "o: onshon, odsol, oshiace, orderis-nd, olber, olrey, onvid, onalo, odbarty, ondolio\n",
            "p: pensens, parlzy, petprilly, phitfaet, ponrail, pohlsio, pronjie, perchen, parnide, pevizor\n",
            "q: qoariaun, qiadveu, qackaen, qceodsee, qarlcarlie, qeorcerne, qhitture, qiuwdord, qeratalo, qhevoszele\n",
            "r: reccie, ry, ruk, rastele, regel, rud, reami, rase, ruccon, rawik\n",
            "s: shasy, salfio, sey, stinen, sendy, shomumes, shylnole, slili, stegof, suy\n",
            "t: tecla, tunsin, torny, ternio, torniodo, tegkeemuw, trweldar, terlkun, tunlinlos, tlack\n",
            "u: ualron, udy, ungey, uullon, uxon, unkea, uarid, ugultan, usurv, uulvan\n",
            "v: vethy, veboun, vakim, vaabor, vilkere, viyclaver, vaoder, vard, veery, vormun\n",
            "w: woynovie, warleo, wandenin, wishy, warri, winpe, wionie, werch, wous, watie\n",
            "x: xvatharale, xayealon, xantenem, xelrey, xatky, xrimbiimoh, xiine, xodeich, xolemes, xebraun\n",
            "y: ybin, ydguin, ynvon, yalbeme, yren, yronce, yriph, ynes, yen, yar\n",
            "z: zeblemo, zalus, zrethris, zavay, zedanorse, zeumialan, zalie, zarnico, zaudy, zedweme\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant regarder si notre générateur a créé des prénoms qui se trouvaient dans nos listes. Notre modèle est intéressant s'il crée quelques prénoms existants, mais pas en majorité. S'il ne crée que des prénoms existants, c'est qu'il a appris notre liste par coeur et est beaucoup trop entrainé."
      ],
      "metadata": {
        "id": "ici9YjIL8TNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_found_names = [name for name in female_names\n",
        "                      if name + \"\\n\"*(female_data.max_len - len(name))\n",
        "                      in female_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(female_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCzziOhN-j5B",
        "outputId": "30f931d5-4f6c-4de2-b2d4-6fd4b85bc5c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ally, britte, carry, dara, elly, harli, kerri, kelli, loree, reta, verla, winni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_found_names = [name for name in male_names\n",
        "                    if name + \"\\n\"*(male_data.max_len - len(name))\n",
        "                    in male_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(male_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfzmgVUa-mew",
        "outputId": "00dc46d8-e67e-4591-b6eb-41c741622620"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Y8zvxjd0CZfX"
      }
    }
  ]
}