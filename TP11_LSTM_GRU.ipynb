{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TsPR9wc5_70_isxcjOj-QL-eBYwzwjqA",
      "authorship_tag": "ABX9TyOXaHBv+e7YLtvdwF05YLML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gguex/ISH_ressources_cours_ML/blob/main/TP11_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP 11 : LSTM et GRU\n",
        "\n",
        "Dans ce TP, nous allons apprendre à créer et entrainer des réseaux de type LSTM et GRU pour créer des modèles génératifs.\n",
        "\n",
        "Dans la première partie, nous allons entrainer un réseau LSTM qui permet de générer du texte \"similaire\" à des données d'entrainement. Nous utiliserons pour cela le premier livre des Misérables.\n",
        "\n",
        "Dans la deuxième partie, nous allons entrainer un réseau GRU qui permet de générer des prénoms féminins ou masculins qui \"sonnent juste\", mais qui n'existent pas nécessairement. Nous utiliserons pour cela le jeu de données de prénoms anglophones trouvé sur http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/ (mais vous pouvez essayer avec d'autres listes de prénoms, ces dernières sont faciles à trouver).\n",
        "\n",
        "Les librairies nécessaires sont les suivantes :"
      ],
      "metadata": {
        "id": "xJftcrf_fBjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqoWHussKYEq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "import string\n",
        "# Permet d'afficher le texte avec une certaine largeur\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enregistrons le dispositif de calcul dans une variable."
      ],
      "metadata": {
        "id": "S8FXdFVtj3Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ksaGk3fkEX-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbf6ff0-13ed-4d44-86ee-d8adb876e83d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Génération de textes avec LSTM\n",
        "\n",
        "Dans cette partie, nous allons créer un modèle de génération de textes qui sera entrainé sur le premier livre des Misérables (cf. TP 10). Ce modèle sera constitué de plusieurs couches LSTM successives, et devra être entrainé à prédire le prochain token d'une séquence en fonction des tokens précédents."
      ],
      "metadata": {
        "id": "eXUpfBThkE4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` pour pouvoir générer des exemples d'entrainement.\n",
        "\n",
        "Un entrée/sortie de ce dataset sera constitué de deux séquences, tirées de notre document, de taille `seq_len`. La séquence de sortie sera décalée d'un token sur la droite par rapport à l'entrée (voir l'exemple plus bas). Ainsi, chaque token de la séquence d'entrée sera associé avec le token suivant.\n",
        "\n",
        "Nous allons utiliser un modèle de langage de Spacy pour tokeniser notre document."
      ],
      "metadata": {
        "id": "y0ATSP1nku7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceData(Dataset):\n",
        "    def __init__(self, file_path, nlp, seq_len):\n",
        "\n",
        "        # Le chemin d'accès au fichier\n",
        "        self.file_path = file_path\n",
        "        # La longueur des séquences d'entrée et sortie\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # On ouvre notre fichier et on crée un object Spacy\n",
        "        with open(self.file_path, \"r\") as f:\n",
        "          text = f.read()\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # On découpe notre texte en tokens, sans les espaces\n",
        "        self.tokens = [token.text for token in doc if not token.is_space]\n",
        "\n",
        "        # On enregistre le vocabulaire utilisé, et on crée des dictionnaires\n",
        "        # permettant de transformer chaque token en identifiant numérique,\n",
        "        # ou l'inverse.\n",
        "        self.dictionary = list(set(self.tokens))\n",
        "        self.id2token = {id: token for id, token in enumerate(self.dictionary)}\n",
        "        self.token2id = {token: id for id, token in enumerate(self.dictionary)}\n",
        "\n",
        "        # On transforme notre corpus en une séquence de valeurs numériques\n",
        "        self.token_ids = [self.token2id[w] for w in self.tokens]\n",
        "\n",
        "    # La taille de notre dataset est le nombre de séquences possibles\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids) - self.seq_len\n",
        "\n",
        "    # Un élément de notre corpus sera la séquence id et la séquence id+1.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.token_ids[id:id+self.seq_len]),\n",
        "                torch.tensor(self.token_ids[id+1:id+self.seq_len+1]))"
      ],
      "metadata": {
        "id": "aBf2WJTKEXEO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On charge le modèle de langage de Spacy."
      ],
      "metadata": {
        "id": "l4tegHb-oKGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !spacy download \"en_core_web_sm\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "nX5JqfXwEjyR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre classe `SentenceData`, en utilisant le premier livre de notre corpus et en le divisant en séquences de 10 tokens."
      ],
      "metadata": {
        "id": "HoC3igPr0MuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/book_01.txt\"\n",
        "example_doc = SentenceData(doc_path, nlp, 10)"
      ],
      "metadata": {
        "id": "4vRKSDCwGmnp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_doc.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75oAaZDVX3FK",
        "outputId": "c908432b-49fb-4a5a-b4f2-ba2c867a5693"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4728"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre document contient 28932 paires de séquences."
      ],
      "metadata": {
        "id": "51HAOH7u0VD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJMYd_sH1XI",
        "outputId": "5296bf54-693a-4701-a41c-d3bcfc2df5fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28932"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut examiner un exemple en particulier, en traduisant les séquences numériques en mots."
      ],
      "metadata": {
        "id": "RX4XIx9v0dpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id = 0\n",
        "input, output = example_doc[sample_id]\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in input]))\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in output]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBb9EO8XHORw",
        "outputId": "19ea905d-c5b2-4ee5-8ca9-6dc4a301466a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 1815 , M. Charles - François - Bienvenu Myriel\n",
            "1815 , M. Charles - François - Bienvenu Myriel was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée un objet `torch.utils.data.Dataloader` pour parcourir notre base de données, avec une taille de batch donnée."
      ],
      "metadata": {
        "id": "lXIhKhPm0nPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "my_dataloader = DataLoader(example_doc, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "rP1RISBZNbzX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est temps de créer notre modèle. Ce dernier prendra notre séquence d'entrée et sera consititué de :\n",
        "\n",
        "* Une couche d'embedding, qui transforme nos identifiants numériques en vecteurs one-hot de taille `n_vocab`, puis en vecteur de taille `embedding_dim`.\n",
        "* `num_layers` couches de LSTM, avec des vecteurs d'état caché et d'état de cellule de taille `hidden_size`. On peut ajouter un dropout aux couches, afin que ces dernières n'apprennent pas le texte entièrement par coeur.\n",
        "* Une couche entièrement connectée, avec `n_vocab` sorties.\n",
        "\n",
        "La sortie de ce réseau donnera les log-odds pour les mots suivants, qui peuvent entre converties en probabilités.\n",
        "\n",
        "Notez que la longueur de la séquence n'a pas besoin d'être précisée ici. Pytorch se chargera de prendre tous les éléments reçus, de \"déplier\" le réseau en fonction de leur nombre, et de calculer les sorties en transmettant les états entre les cellules.\n",
        "\n",
        "Remarquez également que nous précisons avec `batch_first=True`, lors de la création des couches LSTM, que notre batch se situe sur la première dimension de nos tenseurs. Ainsi, Pytorch comprend que la séquence se situe sur la deuxième dimension des tenseurs (voir https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)."
      ],
      "metadata": {
        "id": "cm42-xGY1GmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqGen(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, hidden_size, num_layers):\n",
        "        super(SeqGen, self).__init__()\n",
        "\n",
        "        # On sauve les paramètres dans des attributs\n",
        "        self.n_vocab = n_vocab\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # La couche d'embedding\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=self.n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        # Les couches LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    # La fonction foward peut prendre, en plus des entrées,\n",
        "    # l'état caché et l'état de la cellule utilisé au début de la séquence.\n",
        "    # Par défaut, ces états sont posés comme None.\n",
        "    def forward(self, x, prev_state=None):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "kj65kz_oKnr6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre modèle, avec nombre de mots correspondant à notre vocabulaire, embedding de 128 et 3 couches de LSTM avec états de taille 128."
      ],
      "metadata": {
        "id": "-YhjGZK7_5PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_gen = SeqGen(len(example_doc.dictionary), 128, 128, 3)"
      ],
      "metadata": {
        "id": "bcnKUn8oTO6e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser l'entropie croisée et l'optimisateur Adam pour entrainer notre modèle."
      ],
      "metadata": {
        "id": "LgnEYlT5AQTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(seq_gen.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jPcpxj7OT4VA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On entraine maintenant le modèle. Notez qu'aucun état caché est donné à notre modèle avant chaque batch, ce qui veut dire que les états initiaux seront nuls avant chaque séquence. L'entrainement est relativement simple, car la surparamétrisation n'est pas très importante pour notre modèle."
      ],
      "metadata": {
        "id": "B2581VwZAcd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le nombre d'epochs\n",
        "n_epochs = 30\n",
        "\n",
        "# On met le modèle sur le dispositif de calcul\n",
        "seq_gen.to(device)\n",
        "# On le met en mode entrainement\n",
        "seq_gen.train()\n",
        "# On boucle sur les epochs\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "  # Pour calculer la perte moyenne\n",
        "  sum_loss = 0\n",
        "  # On boucle sur notre dataloader\n",
        "  for input, output in my_dataloader:\n",
        "\n",
        "    # Les entrées et sorties sont mises sur le dispositif de calcul\n",
        "    input = input.to(device)\n",
        "    output = output.to(device)\n",
        "\n",
        "    # On met à zéro les gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # On fait les prédictions\n",
        "    pred, _ = seq_gen(input)\n",
        "    # On calcule la perte, en transposant nos résultats\n",
        "    loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "    # On fait une itération de descente du gradient\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # On ajoute la perte\n",
        "    sum_loss += loss.item()\n",
        "\n",
        "  print(f\"mean loss = {sum_loss / len(my_dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrWiGXq-T7ty",
        "outputId": "037c46d2-0be1-4de5-e0eb-af292a4f5d4d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean loss = 6.7390\n",
            "Epoch 2: mean loss = 6.2644\n",
            "Epoch 3: mean loss = 6.1936\n",
            "Epoch 4: mean loss = 6.0622\n",
            "Epoch 5: mean loss = 5.8631\n",
            "Epoch 6: mean loss = 5.6746\n",
            "Epoch 7: mean loss = 5.4695\n",
            "Epoch 8: mean loss = 5.2838\n",
            "Epoch 9: mean loss = 5.1071\n",
            "Epoch 10: mean loss = 4.9460\n",
            "Epoch 11: mean loss = 4.8030\n",
            "Epoch 12: mean loss = 4.6704\n",
            "Epoch 13: mean loss = 4.5487\n",
            "Epoch 14: mean loss = 4.4354\n",
            "Epoch 15: mean loss = 4.3282\n",
            "Epoch 16: mean loss = 4.2286\n",
            "Epoch 17: mean loss = 4.1299\n",
            "Epoch 18: mean loss = 4.0389\n",
            "Epoch 19: mean loss = 3.9529\n",
            "Epoch 20: mean loss = 3.8766\n",
            "Epoch 21: mean loss = 3.8032\n",
            "Epoch 22: mean loss = 3.7316\n",
            "Epoch 23: mean loss = 3.6574\n",
            "Epoch 24: mean loss = 3.5855\n",
            "Epoch 25: mean loss = 3.5166\n",
            "Epoch 26: mean loss = 3.4545\n",
            "Epoch 27: mean loss = 3.3943\n",
            "Epoch 28: mean loss = 3.3321\n",
            "Epoch 29: mean loss = 3.2715\n",
            "Epoch 30: mean loss = 3.2071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va maintenant donner à notre modèle un début de séquence et voir comment il génère une suite. Le processus se passe en deux étapes :\n",
        "\n",
        "* On passe une première fois la séquence initiale dans notre modèle, afin de générer le prochain token et pour récupérer l'état caché résultant.\n",
        "* On va ensuite faire une boucle pour les tokens restants, en passant dans le réseau le dernier token et états obtenus à l'étape précédente.\n",
        "\n",
        "Pour générer chaque nouveau token, on transforme les log-odds en probabilités, puis on effectue un tirage aléatoire selon ces dernières.\n",
        "\n",
        "Le séquence finale est affichée formattée."
      ],
      "metadata": {
        "id": "P2pM7sEdGYaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Séquence initiale\n",
        "input_sentence = \"When the man\"\n",
        "# Nombre de tokens désiré\n",
        "n_generated_tokens = 600\n",
        "\n",
        "# --- Etape 1\n",
        "\n",
        "# On coupe notre séquence intiale pour créer notre séquence de sortie\n",
        "output_tokens = input_sentence.split()\n",
        "# On met le modèle en mode évaluation\n",
        "seq_gen.eval()\n",
        "# Notre séquence d'entrée est transformée en identifiants numériques\n",
        "input = torch.tensor([example_doc.token2id[token]\n",
        "                      for token in output_tokens]).to(device)\n",
        "# On passe nos entrées dans notre modèle, en ne donnant aucun état.\n",
        "pred, hidden = seq_gen(input)\n",
        "# On garde les logodds du dernier mot. Notez que nous utilisons\n",
        "# detach().cpu() pour ne pas garder le graph de calcul et pour\n",
        "# basculer le tenseur sur le cpu.\n",
        "new_token_logodds = pred.detach().cpu()[-1]\n",
        "# Les probabilités sont calculées avec un softmax\n",
        "probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "# On tire aléatoirement le token suivant, avec les probabilités précédentes\n",
        "token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "# On l'ajoute à notre sortie\n",
        "output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# --- Etape 2\n",
        "\n",
        "# On boucle sur le nombre restant de tokens demandés\n",
        "for i in range(n_generated_tokens - 1):\n",
        "  # Notre dernier token est transformé en entrée\n",
        "  input = torch.tensor([example_doc.token2id[output_tokens[-1]]]).to(device)\n",
        "  # On passe notre entrée, avec les états précédants, dans notre modèle\n",
        "  pred, hidden = seq_gen(input, hidden)\n",
        "  # On détache, met sur le cpu et applatit la sortie\n",
        "  new_token_logodds = pred.detach().cpu().flatten()\n",
        "  # Les probabilités sont calculées avec un softmax\n",
        "  probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "  # On tire aléatoirement, avec les probabilités calculées, le token suivant\n",
        "  token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "  # On l'ajoute à notre sortie\n",
        "  output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# On affiche notre sortie formatée\n",
        "print(textwrap.fill(\" \".join(output_tokens), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0JzgJA9VfIk",
        "outputId": "1f96a31d-650d-413f-aade-1a963a716ac0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the man of his curé . ” He had occupied the real words ; whom can not\n",
            "peculiar ! There are people somewhat ? I call him home with the mountain of\n",
            "them for a bishop ! There like turn alone are contents in retirement : — He\n",
            "examined without long sides of destiny , and who have reflection here . It had\n",
            "its conferred before his wisdom upon in the grave absence of him developed\n",
            "soles the abysses of the army of the chimney . But each cause obliged to notary\n",
            "over a heart at the poor . _ _ _ diocese _ was been Matters ! Thou on human\n",
            "bearings , which night about from the lesson and combat pursued in children ,\n",
            "and dissolved than ox de Lô , passes by civilization to see himself , as how by\n",
            "dangerous through through better , nevertheless , prayer , such guard ; like\n",
            "any fatal latter unfold something by them , and even with a wretch , but\n",
            "vocation dwells out on three or dispensations , some away there , in drops of\n",
            "St. one communicate in the stirrup for another ; and was perfectly features\n",
            "bent with that revolutionists indulgent enclosure , in Arles , and seeking to\n",
            "adore priests on the presence of the king , and to speak Welcome is not by his\n",
            "cardinalship , but recognized,—that a little of ” _ replied the Bishop , sir .\n",
            "That and just according we probably from going to public promotions . He roll\n",
            "in his sons ’s hand , which was venerated ; there were whither the complexion\n",
            "of a infant by his own ] . Who are my apothecary revere in his seventy - Meuse\n",
            "; he applauds himself , as it was on the want ; power ; like religion from\n",
            "Mademoiselle Baptistine . There are wear them with many table , on people ,\n",
            "before not arouse him up by all with that tired , Monsieur ! _ hollows took a\n",
            "cold manner , which is evidently no fitting and but reached Fesch . At their\n",
            "bench . “ _ ideas , ” said he , and pressed from his eyes from his speedy\n",
            "eighty , entered bishops , though perfectly nonsense , full where this speeches\n",
            "her disappeared , a drawing who raised not replied . A dogma of nature , and\n",
            "contemplation in egotism - haired gayety , at these usurp the abyss of the\n",
            "Imperial Council of Senez , above when exactly bread , as time into rainy by\n",
            "things : — NOTE reply over less it is to be . concerned , it must not one ; but\n",
            "any poverty , and rather rather slowly by its widely there ; sometimes even\n",
            "indestructible . Far for his men . such multum false problems buildings as\n",
            "incessantly at these plump , and ; under awaiting groans and beggars , after\n",
            "allowance ; the Emperor are a man for a deceives ; the isolation of D — — .\n",
            "respect , these moments instinct from mention a Eminence , one softened cause\n",
            "to this mistake loved , and with a vine cupboard . Not his encounter , which is\n",
            "transcribe themselves slipped asleep , in absolute - Vence ; amavit_,—because ;\n",
            "this on them : he must consider a heads preoccupation . Considerable can\n",
            "suddenly in beholding with a thoughtless fashion on his anchylosis and\n",
            "sufficient on applaud ; also father ? actually blew , but ? Thou for a\n",
            "ultramontane as though we have feared him over once here that she speaking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "weevWLMZ9bQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Génération de prénoms avec GRU\n",
        "\n",
        "Maintenant, nous allons utiliser les GRU pour créer des modèles de génération de prénoms, en utilisant des données de prénoms féminins et masculins. Le principe reste assez similaire à la partie précédente, mais nous adopterons une programmation de style plus fonctionnel."
      ],
      "metadata": {
        "id": "_zoqTHMJLtZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` qui va donner des exemples constitués d'un nom comme entrée et du même nom décalé d'une lettre sur la droite comme sortie. Comme caractère de fin de séquence et de padding, nous allons utiliser le caractère `\"\\n\"` déjà présent à la fin de chaque nom dans le fichier (sauf du dernier). Nous effectuons un padding sur tous les prénoms afin que tous fassent la taille du plus long prénom (pour entrainer les modèles avec des batchs)."
      ],
      "metadata": {
        "id": "qPy0pb8TMCsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameData(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        # On enregistre le chemin du fichier\n",
        "        self.file_path = file_path\n",
        "        # On ouvre le fichier et on lit les lignes\n",
        "        with open(file_path, \"r\") as f:\n",
        "          self.lines = f.readlines()\n",
        "        # On ajoute notre caractère de padding à la dernière ligne\n",
        "        self.lines[-1] += \"\\n\"\n",
        "        # La longueur maximale est la longueur de la plus grande ligne\n",
        "        self.max_len = max([len(line) for line in self.lines])\n",
        "        # Les lignes sont complétées avec \"\\n\" pour avoir la même taille\n",
        "        self.lines = [line + \"\\n\"*(self.max_len - len(line))\n",
        "                      for line in self.lines]\n",
        "        # On met les lignes en minuscules\n",
        "        self.lines_low = [line.lower() for line in self.lines]\n",
        "        # On sauve le nombre de lettres différentes\n",
        "        self.letters = list(set(\"\".join(self.lines_low)))\n",
        "        # On crée notre liste de nom, avec chaque lettre\n",
        "        # comme élément d'une liste\n",
        "        self.names = [list(line) for line in self.lines_low]\n",
        "\n",
        "        # Ces fonctions permettent de transformer les lettres en identifiants\n",
        "        # numériques, ou vice-versa.\n",
        "        self.id2letter = {id: letter for id, letter in enumerate(self.letters)}\n",
        "        self.letter2id = {letter: id for id, letter in enumerate(self.letters)}\n",
        "\n",
        "        # On sauve l'identifiant du caractère de padding\n",
        "        self.endl_id = self.letter2id[\"\\n\"]\n",
        "\n",
        "        # On transforme nos listes de lettres en listes d'ids\n",
        "        self.names_ids = [[self.letter2id[l] for l in name]\n",
        "                          for name in self.names]\n",
        "\n",
        "    # La longueur du dataset\n",
        "    def __len__(self):\n",
        "        return len(self.names_ids)\n",
        "\n",
        "    # Un item est une entrée/sortie avec, respectivement,\n",
        "    # prénom sans la dernière lettre, prénom sans la première lettre.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.names_ids[id][:-1]),\n",
        "                torch.tensor(self.names_ids[id][1:]))"
      ],
      "metadata": {
        "id": "2PLVeGU39cC4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée maintenant nos instances contenant nos jeux de données."
      ],
      "metadata": {
        "id": "BpFQh1DkOrJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/female.txt\"\n",
        "male_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/male.txt\"\n",
        "female_data = NameData(female_path)\n",
        "male_data = NameData(male_path)"
      ],
      "metadata": {
        "id": "ukgFs2NC9yMT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons un exemple."
      ],
      "metadata": {
        "id": "9OLmhvMiO0ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, output = female_data[0]\n",
        "print([female_data.id2letter[id.item()] for id in input])\n",
        "print([female_data.id2letter[id.item()] for id in output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYDL5IPb94bZ",
        "outputId": "2dc00664-66b3-4201-d167-e3d7241d42ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n",
            "['b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous créons maintenant la classe de notre modèle. Ce dernier passe  directement les vecteurs one-hot dans les couches GRU, car le nombre de caractères existants n'est pas très élevé (la dimensionnalité des vecteurs one-hot est déjà basse). Le modèle est constitué de :\n",
        "\n",
        "* `num_layer` couches GRU avec états cachés de taille `hidden_size`.\n",
        "* Une couche entièrement connectée qui envoie les états cachés finaux sur la taille du vocabulaire.\n",
        "\n",
        "Le résultat du modèle sont les log-odds des prochains caractères."
      ],
      "metadata": {
        "id": "XE6CYho4jfOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameGen(nn.Module):\n",
        "    def __init__(self, n_vocab, hidden_size, num_layers):\n",
        "        super(NameGen, self).__init__()\n",
        "\n",
        "        # On enregistre les paramètres du modèle\n",
        "        self.n_vocab = n_vocab\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Les couches GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.n_vocab,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "        # Cette fonction transforme les entiers en vecteurs one-hot\n",
        "        one_hot = nn.functional.one_hot(x, num_classes=self.n_vocab)\n",
        "        # On passe dans les couches GRU\n",
        "        output, state = self.gru(one_hot.to(torch.float), prev_state)\n",
        "        # Les log-odds résultantes\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "zQ0fLqLc-Eab"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous instancions un modèle et lui passons un exemple, pour voir si tout fonctionne."
      ],
      "metadata": {
        "id": "ZjaDToc6lQWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 20, 2)\n",
        "female_name_gen(input)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV5nAhek-FDs",
        "outputId": "fa6c9c37-9579-4691-a7a2-edf577e9acb7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1525,  0.0187,  0.1557, -0.0312, -0.1576,  0.0762,  0.1568, -0.2207,\n",
              "          0.1727,  0.0535, -0.1775, -0.1216, -0.2168, -0.2126, -0.1041,  0.1048,\n",
              "          0.2057,  0.0285, -0.1031, -0.1937, -0.1469, -0.2782,  0.0938, -0.1060,\n",
              "         -0.0197, -0.1146, -0.1857, -0.1443,  0.0711, -0.0602],\n",
              "        [-0.1543,  0.0127,  0.1475, -0.0197, -0.1446,  0.0368,  0.1619, -0.2451,\n",
              "          0.1909,  0.0995, -0.1828, -0.1147, -0.2455, -0.2071, -0.1009,  0.1080,\n",
              "          0.1916, -0.0157, -0.0839, -0.1788, -0.1155, -0.3051,  0.0761, -0.0946,\n",
              "         -0.0123, -0.1040, -0.1766, -0.1714,  0.0772, -0.0876],\n",
              "        [-0.1559,  0.0020,  0.1498, -0.0252, -0.1408,  0.0254,  0.1707, -0.2644,\n",
              "          0.2098,  0.1210, -0.1947, -0.1193, -0.2510, -0.2095, -0.0927,  0.1113,\n",
              "          0.1968, -0.0462, -0.0740, -0.1774, -0.1206, -0.3371,  0.0588, -0.1030,\n",
              "         -0.0069, -0.1005, -0.1735, -0.1747,  0.0956, -0.0884],\n",
              "        [-0.1474,  0.0017,  0.1468, -0.0187, -0.1346,  0.0184,  0.1656, -0.2593,\n",
              "          0.2200,  0.1221, -0.1874, -0.1276, -0.2510, -0.2035, -0.0822,  0.1166,\n",
              "          0.1982, -0.0699, -0.0773, -0.1766, -0.1130, -0.3421,  0.0564, -0.1139,\n",
              "         -0.0092, -0.1061, -0.1600, -0.1798,  0.0893, -0.0919],\n",
              "        [-0.1510, -0.0072,  0.1452, -0.0268, -0.1354,  0.0209,  0.1648, -0.2654,\n",
              "          0.2311,  0.1254, -0.1918, -0.1294, -0.2454, -0.2094, -0.0770,  0.1201,\n",
              "          0.2023, -0.0786, -0.0712, -0.1789, -0.1266, -0.3552,  0.0526, -0.1202,\n",
              "         -0.0062, -0.1082, -0.1592, -0.1750,  0.1011, -0.0830],\n",
              "        [-0.1398, -0.0306,  0.1393, -0.0323, -0.1634,  0.0355,  0.1598, -0.2690,\n",
              "          0.2448,  0.1232, -0.2149, -0.1172, -0.2336, -0.2382, -0.0806,  0.1302,\n",
              "          0.2032, -0.0858, -0.0667, -0.1931, -0.1425, -0.3630,  0.0653, -0.1490,\n",
              "         -0.0218, -0.1214, -0.1556, -0.1624,  0.1154, -0.0545],\n",
              "        [-0.1341, -0.0300,  0.1321, -0.0420, -0.1754,  0.0518,  0.1580, -0.2875,\n",
              "          0.2534,  0.1268, -0.2113, -0.1309, -0.2279, -0.2645, -0.0902,  0.1224,\n",
              "          0.2021, -0.0878, -0.0658, -0.1961, -0.1406, -0.3775,  0.0678, -0.1552,\n",
              "         -0.0045, -0.1205, -0.1705, -0.1470,  0.1415, -0.0499],\n",
              "        [-0.1294, -0.0379,  0.1539, -0.0456, -0.1600,  0.0642,  0.1527, -0.2898,\n",
              "          0.2367,  0.1216, -0.2024, -0.1328, -0.2308, -0.2507, -0.0846,  0.1216,\n",
              "          0.2289, -0.0854, -0.0904, -0.1943, -0.1275, -0.3750,  0.0462, -0.1579,\n",
              "         -0.0053, -0.1329, -0.1843, -0.1578,  0.1387, -0.0756],\n",
              "        [-0.1296, -0.0451,  0.1752, -0.0476, -0.1440,  0.0724,  0.1495, -0.2894,\n",
              "          0.2196,  0.1144, -0.1957, -0.1297, -0.2312, -0.2293, -0.0729,  0.1232,\n",
              "          0.2555, -0.0828, -0.1160, -0.1906, -0.1178, -0.3733,  0.0222, -0.1636,\n",
              "         -0.0122, -0.1471, -0.1880, -0.1744,  0.1309, -0.0993],\n",
              "        [-0.1321, -0.0499,  0.1901, -0.0484, -0.1331,  0.0772,  0.1479, -0.2888,\n",
              "          0.2071,  0.1079, -0.1916, -0.1257, -0.2293, -0.2112, -0.0620,  0.1253,\n",
              "          0.2744, -0.0804, -0.1354, -0.1875, -0.1126, -0.3733,  0.0037, -0.1702,\n",
              "         -0.0194, -0.1588, -0.1853, -0.1885,  0.1243, -0.1147],\n",
              "        [-0.1349, -0.0527,  0.1994, -0.0484, -0.1269,  0.0797,  0.1472, -0.2881,\n",
              "          0.1990,  0.1030, -0.1893, -0.1225, -0.2266, -0.1985, -0.0541,  0.1272,\n",
              "          0.2859, -0.0783, -0.1485, -0.1855, -0.1105, -0.3736, -0.0084, -0.1757,\n",
              "         -0.0251, -0.1672, -0.1804, -0.1983,  0.1196, -0.1231],\n",
              "        [-0.1370, -0.0543,  0.2048, -0.0477, -0.1238,  0.0810,  0.1469, -0.2873,\n",
              "          0.1940,  0.0997, -0.1880, -0.1205, -0.2241, -0.1902, -0.0491,  0.1289,\n",
              "          0.2922, -0.0766, -0.1567, -0.1845, -0.1098, -0.3737, -0.0155, -0.1797,\n",
              "         -0.0290, -0.1726, -0.1755, -0.2043,  0.1167, -0.1271],\n",
              "        [-0.1384, -0.0552,  0.2079, -0.0468, -0.1223,  0.0817,  0.1468, -0.2864,\n",
              "          0.1910,  0.0975, -0.1872, -0.1194, -0.2222, -0.1851, -0.0463,  0.1304,\n",
              "          0.2954, -0.0752, -0.1617, -0.1843, -0.1098, -0.3735, -0.0194, -0.1823,\n",
              "         -0.0316, -0.1759, -0.1716, -0.2077,  0.1148, -0.1288],\n",
              "        [-0.1392, -0.0557,  0.2096, -0.0459, -0.1217,  0.0821,  0.1467, -0.2856,\n",
              "          0.1891,  0.0961, -0.1867, -0.1188, -0.2209, -0.1820, -0.0447,  0.1315,\n",
              "          0.2969, -0.0741, -0.1646, -0.1844, -0.1100, -0.3730, -0.0213, -0.1839,\n",
              "         -0.0333, -0.1778, -0.1686, -0.2096,  0.1136, -0.1292],\n",
              "        [-0.1396, -0.0560,  0.2106, -0.0451, -0.1215,  0.0824,  0.1466, -0.2849,\n",
              "          0.1879,  0.0953, -0.1864, -0.1187, -0.2201, -0.1801, -0.0440,  0.1324,\n",
              "          0.2975, -0.0733, -0.1663, -0.1846, -0.1103, -0.3724, -0.0222, -0.1848,\n",
              "         -0.0343, -0.1789, -0.1666, -0.2106,  0.1129, -0.1292]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'idée de prendre une approche fonctionnelle, nous définissons ici la fonction qui permet d'entrainer un modèle."
      ],
      "metadata": {
        "id": "idkOpfztle6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data, batch_size, n_epochs, device):\n",
        "\n",
        "  # On définit la fonction de perte et l'optimisateur\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  # On crée le dataloader\n",
        "  dataloader = DataLoader(data, batch_size, shuffle=True)\n",
        "\n",
        "  # On bascule le modèle en mode entrainement\n",
        "  model.train()\n",
        "  # On le met sur le dispositif de calcul\n",
        "  model.to(device)\n",
        "  # La boucle d'entrainement\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "    # Pour calculer la perte moyenne\n",
        "    sum_loss = 0\n",
        "    # La boucle sur les batchs\n",
        "    for input, output in dataloader:\n",
        "\n",
        "      # On met l'entrée et la sortie sur le dispositif de calcul\n",
        "      input = input.to(device)\n",
        "      output = output.to(device)\n",
        "\n",
        "      # On met les gradients à zéro, on fait des prédictions et on\n",
        "      # calcule la perte\n",
        "      optimizer.zero_grad()\n",
        "      pred, _ = model(input)\n",
        "      loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "      # On effectue la descente du gradient et on cumule les pertes\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      sum_loss += loss.item()\n",
        "\n",
        "    print(f\"mean Loss = {sum_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "s1xFZYsw-L1S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise maintenant la fonction créée pour entrainer un modèle avec les prénoms féminins."
      ],
      "metadata": {
        "id": "WU9xzgalmgBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 30, 3)\n",
        "train_model(female_name_gen, female_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUUZldXv-Q08",
        "outputId": "38c1d9fa-73e6-4e68-a27f-dff47fc5a4eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.3260\n",
            "Epoch 2: mean Loss = 0.9806\n",
            "Epoch 3: mean Loss = 0.9114\n",
            "Epoch 4: mean Loss = 0.8795\n",
            "Epoch 5: mean Loss = 0.8618\n",
            "Epoch 6: mean Loss = 0.8486\n",
            "Epoch 7: mean Loss = 0.8356\n",
            "Epoch 8: mean Loss = 0.8254\n",
            "Epoch 9: mean Loss = 0.8133\n",
            "Epoch 10: mean Loss = 0.8007\n",
            "Epoch 11: mean Loss = 0.7882\n",
            "Epoch 12: mean Loss = 0.7760\n",
            "Epoch 13: mean Loss = 0.7653\n",
            "Epoch 14: mean Loss = 0.7566\n",
            "Epoch 15: mean Loss = 0.7488\n",
            "Epoch 16: mean Loss = 0.7412\n",
            "Epoch 17: mean Loss = 0.7345\n",
            "Epoch 18: mean Loss = 0.7279\n",
            "Epoch 19: mean Loss = 0.7219\n",
            "Epoch 20: mean Loss = 0.7162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "eg4j_PXPmpOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_name_gen = NameGen(len(male_data.letters), 30, 3)\n",
        "train_model(male_name_gen, male_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PthBaFgw-TIH",
        "outputId": "d047d29d-b419-4769-fe2e-92568c4f0822"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.5525\n",
            "Epoch 2: mean Loss = 1.0975\n",
            "Epoch 3: mean Loss = 1.0424\n",
            "Epoch 4: mean Loss = 1.0226\n",
            "Epoch 5: mean Loss = 1.0107\n",
            "Epoch 6: mean Loss = 0.9961\n",
            "Epoch 7: mean Loss = 0.9730\n",
            "Epoch 8: mean Loss = 0.9408\n",
            "Epoch 9: mean Loss = 0.9227\n",
            "Epoch 10: mean Loss = 0.9151\n",
            "Epoch 11: mean Loss = 0.9080\n",
            "Epoch 12: mean Loss = 0.9013\n",
            "Epoch 13: mean Loss = 0.8936\n",
            "Epoch 14: mean Loss = 0.8859\n",
            "Epoch 15: mean Loss = 0.8782\n",
            "Epoch 16: mean Loss = 0.8703\n",
            "Epoch 17: mean Loss = 0.8643\n",
            "Epoch 18: mean Loss = 0.8568\n",
            "Epoch 19: mean Loss = 0.8518\n",
            "Epoch 20: mean Loss = 0.8453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons maintenant une fonction qui permet de générer un prénom en fonction des premiers caractères. La procédure est très similaire à la génération de tokens, mais on va arrêter la génération dès que notre modèle prédit le caractère \"\\n\", signifiant la fin de séquence."
      ],
      "metadata": {
        "id": "sepYp7C9snFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_name(init_letters, model, data, device):\n",
        "\n",
        "  # --- Etape 1\n",
        "\n",
        "  output_letters = list(init_letters)\n",
        "  model.eval()\n",
        "  input = torch.tensor([data.letter2id[letter]\n",
        "                        for letter in output_letters]).to(device)\n",
        "  pred, hidden = model(input)\n",
        "  new_letter_logodds = pred.detach().cpu()[-1]\n",
        "  probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "  next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "  output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  # --- Etape 2\n",
        "\n",
        "  # On boucle tant que l'on a pas généré \"\\n\"\n",
        "  while not output_letters[-1] == \"\\n\":\n",
        "    input = torch.tensor([data.letter2id[output_letters[-1]]]).to(device)\n",
        "    pred, hidden = model(input, hidden)\n",
        "    new_letter_logodds = pred.detach().cpu().flatten()\n",
        "    probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "    next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "    output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  return \"\".join(output_letters)[:-1]"
      ],
      "metadata": {
        "id": "m_YmXApw-WpE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testons la fonction de génération avec un début de prénom."
      ],
      "metadata": {
        "id": "X_cRCEZB72vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_name(\"su\", female_name_gen, female_data, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OWuQx8Urr_Qo",
        "outputId": "dfccb682-51bf-4fcd-94fa-61f7d34849ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sustie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons une liste conséquente de prénoms féminins, pour toutes les lettres de l'alphabet."
      ],
      "metadata": {
        "id": "NA412zGe8EVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "female_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_female_names = []\n",
        "  for _ in range(n_gen):\n",
        "    female_name = generate_name(init_letter, female_name_gen,\n",
        "                                female_data, device)\n",
        "    letter_female_names.append(female_name)\n",
        "\n",
        "  female_names.extend(letter_female_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_female_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpF-Pgip-Y2Y",
        "outputId": "57e34fdc-ce40-493e-9c90-52d36635c1d8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: aulesa, avatyn, aurimi, alis, aulon, angelin, alita, arle, alessanica, adwer\n",
            "b: bilin, brerti, beolette, brichinette, bendi, brette, bandie, bensu, berna, bryora\n",
            "c: charlardki, chila, chrifie, cespi, carinet, cansitte, chan, cay, carlott, cashy\n",
            "d: dubilea, dore, dazine, deada, dasyryl, dia, dadera, dilly, driel, deta\n",
            "e: eolrah, elis, elauna, evrina, elys, eledina, eadace, edawn, elmy, elidell\n",
            "f: frixik, forette, floissy, flora, fralme, farorie, fabrry, festi, fraca, fipharlea\n",
            "g: glerrina, garni, gueena, gepvalie, gochalle, gilli, gienda, gaxbie, gorrella, galissa\n",
            "h: hillisne, hakema, helith, harricte, horty, hanetta, helimanne, hellee, hristi, hii-ancie\n",
            "i: issie, ina, inickah, itpah, ilessa, ivy, inolra, io, idral, iutha\n",
            "j: jiannel, jelnie, jena, juannia, janha, jonaline, jaxelle, joyta, ja, jixic\n",
            "k: karylh, kyele, karie, kyolla, kaltia, kathalyna, karolyn, keleen, kalynse, katalle\n",
            "l: lelca, loudie, llodine, linelle, lisaman, loore, londiy, leredine, lara, lubica\n",
            "m: melnedtepma, mludida, marjyan, melebine, margona, mivestey, melbie, martryca, migni, marea\n",
            "n: nasti, nivelinte, nachette, nacha, nio, nehlhec, netalone, ninca-randa, neria, nengie\n",
            "o: olercu, ongen, oshenta, orela, oykelle, olela, orelaina, omery, oxeta, olena\n",
            "p: parlynne, parlee, pimalin, pristha, parin, pychyn, phybe, pcaur a, pretta, ptotee\n",
            "q: quannelle, quisa, quikelle, quan, quandie, quareanne, qluillene, qouiljah, quelli, querilia\n",
            "r: rixellia, rolan, rovee, rhie, rona, rtona, rafette, romajie, rysh, rloie\n",
            "s: shey, sheliferte, shery, shelnda, slerrie, sargel, shan, solagande, sherrah, sherleenne\n",
            "t: tomithen, thailei, teigronne, toelina, tapterona, tlonie, ttetline, tendy, tathace, tarlena\n",
            "u: ulina, ulebee, umrimlia, ude, ufanie, urna, ugorla, ulissne, ulleri, urgie\n",
            "v: veria, veretta, vylliana, vory, varl, vad, vori, vetdone, veilene, vornisla\n",
            "w: waenciquy, wynda, wwane, wenn, wenn, walo, wynnell, weni, wot, wra\n",
            "x: xinnie, xanryl, xelrie, xelura, xelly, xergary, xanna, xerica, xeneth, xora\n",
            "y: yotrice, ytasna, ybi, yttina, yletta, yee, ynaverla, yvin, ynondi, ydey\n",
            "z: zanetta, zer, ziane, zysnka, zoela, zacha, zancat, zedma, zonshhy, zeisse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "XajfC7vu8PtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "male_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_male_names = []\n",
        "  for _ in range(n_gen):\n",
        "    male_name = generate_name(init_letter, male_name_gen,\n",
        "                              male_data, device)\n",
        "    letter_male_names.append(male_name)\n",
        "\n",
        "  male_names.extend(letter_male_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_male_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9LK-nF6-bHb",
        "outputId": "c8379a5f-24f3-46fd-9ded-ad5ea1309dda"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: akeroy, arao, ale, anfy, agras, aulmio, alsos, arcen, arbige, acrartty\n",
            "b: beven, beollo, byrne, bais, brabes, burbel, barrick, berby, bacwmorke, brorter\n",
            "c: ciffiece, caret, cenzie, chinvolst, chaphud, condando, catie, chinfie, chagbiin, cadie\n",
            "d: dreiy, dora, ditcs, dudtostey, dimald, dade, dieb, daarsre, dolis, dereflodiid\n",
            "e: erawos, eprerlad, erdar, elveco, elrey, egy, eil, edueve, eomeray, exdonfe\n",
            "f: frend, forilt, fra, fuos, frato, fuylt, freyd, fnalen, fufnon, fhalon\n",
            "g: gerby, giit, gomt, gerlan, gesrune, gillault, gembey, gibaze, gebie, garnle\n",
            "h: hardolipg, horttor, hysin, hyhy, holpoy, hanf, hiffony, hanlie, horcet, hinlel\n",
            "i: innonet, izcor, igtaunt, illanon, ilb, invid, illes, izwoi, ickorin, ithar\n",
            "j: jlinmomon, jrar, jaycro, jardo, jomerie, jeuiko, jar, jeb, jamun, jmain\n",
            "k: keun, karein, kissorion, kmarvaiil, kinry, kus, kislels, ken, kynlyn, kimins\n",
            "l: lys, lont, lricly, loomer, lig, limlias, lloch, ladie, lertho, lisemt\n",
            "m: morbor, mott, meubles, mubautoy, minton, miliigy, meerege, marosa, moince, miskun\n",
            "n: noldeld, nak, narmarj, nda, nanke, nait, nico, narie, noillo, naone\n",
            "o: olphas, ozy, ontel, oviphy, odonl, oa, onnend, onkaden, osech, oskeit\n",
            "p: phew, pherto, puyner, piflic, pearmie, pafshestawd, pigt, permiinf, podand, pvainie\n",
            "q: qoskono, qars, qigey, qabe, qaschuith, qaze, quunhon, qiicke, qihudie, qerlin\n",
            "r: raytart, rulleis, rale, rlamcel, rongert, rapue, rel, rolny, rinniigh, rhisgas\n",
            "s: shinnie, slajue, shevorme, selle, sun, shantanade, spalsy, sheiglon, shumn, sartain\n",
            "t: tkar, tarchan, titt, tott, talyy, ttancer, tcadreil, ttarmos, turrol, thindicqae\n",
            "u: ulfy, utanopo, ulru, urne, uzcie, ulmay, unb, ulvy, umalo, uocuil\n",
            "v: vanneth, vollete, vhaso, vibst, vetdie, vorwol, verh, vadhy, vage, vahe\n",
            "w: wartechie, wert, wany, widbialgd, wrcket, waub, wasvis, wamran, werfer, wenvin\n",
            "x: xow, xinhondo, xamie, xinmatt, xedes, xarnie, xakeaf, xend, xedre, xaney\n",
            "y: y, yris, yllaso, ytiles, yckanmago, yst, yky, yle, yslicor, yauav\n",
            "z: zor, zristel, zyar, zant, zason, zutey, zaynin, zaotamor, zadt, zete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant regarder si notre générateur a créé des prénoms qui se trouvaient dans nos listes. Notre modèle est intéressant s'il crée quelques prénoms existants, mais pas en majorité. S'il ne crée que des prénoms existants, c'est qu'il a appris notre liste par coeur et est beaucoup trop entrainé."
      ],
      "metadata": {
        "id": "ici9YjIL8TNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_found_names = [name for name in female_names\n",
        "                      if name + \"\\n\"*(female_data.max_len - len(name))\n",
        "                      in female_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(female_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCzziOhN-j5B",
        "outputId": "e63d1b8b-b947-4232-b067-eadb2b42a0f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alis, berna, dore, flora, gilli, issie, ina, ivy, jena, karie, lara, rona\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_found_names = [name for name in male_names\n",
        "                    if name + \"\\n\"*(male_data.max_len - len(name))\n",
        "                    in male_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(male_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfzmgVUa-mew",
        "outputId": "e03b4722-8656-4246-d95a-646558a7070e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jeb, ken, nico\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Y8zvxjd0CZfX"
      }
    }
  ]
}